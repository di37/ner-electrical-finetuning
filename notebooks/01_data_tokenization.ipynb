{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing to the main directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from transformers import BertTokenizerFast\n",
    "from datasets import load_dataset\n",
    "\n",
    "from utilities import MODEL_ID, DATASET_ID, OUTPUT_DATASET_PATH\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the electrical NER dataset, which contains text, tokens and corresponding entity tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'tokens', 'ner_tags'],\n",
      "        num_rows: 12076\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'tokens', 'ner_tags'],\n",
      "        num_rows: 1509\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'tokens', 'ner_tags'],\n",
      "        num_rows: 1510\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "electrical_ner_dataset = load_dataset(DATASET_ID, trust_remote_code=True)\n",
    "print(electrical_ner_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': (12076, 3), 'validation': (1509, 3), 'test': (1510, 3)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "electrical_ner_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Using a Multimeter, the technician measured the 10 kΩ resistance of a Copper wire in the circuit.',\n",
       " 'tokens': ['Using',\n",
       "  'a',\n",
       "  'Multimeter',\n",
       "  ',',\n",
       "  'the',\n",
       "  'technician',\n",
       "  'measured',\n",
       "  'the',\n",
       "  '10',\n",
       "  'kΩ',\n",
       "  'resistance',\n",
       "  'of',\n",
       "  'a',\n",
       "  'Copper',\n",
       "  'wire',\n",
       "  'in',\n",
       "  'the',\n",
       "  'circuit',\n",
       "  '.'],\n",
       " 'ner_tags': [0, 0, 7, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 5, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "electrical_ner_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence(feature=ClassLabel(names=['O', 'B-COMPONENT', 'I-COMPONENT', 'B-DESIGN_PARAM', 'I-DESIGN_PARAM', 'B-MATERIAL', 'I-MATERIAL', 'B-EQUIPMENT', 'I-EQUIPMENT', 'B-TECHNOLOGY', 'I-TECHNOLOGY', 'B-SOFTWARE', 'I-SOFTWARE', 'B-STANDARD', 'I-STANDARD', 'B-VENDOR', 'I-VENDOR', 'B-PRODUCT', 'I-PRODUCT'], id=None), length=-1, id=None)\n"
     ]
    }
   ],
   "source": [
    "print(electrical_ner_dataset[\"train\"].features[\"ner_tags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(electrical_ner_dataset['train'].description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize and align labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(MODEL_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to tokenize text and align NER labels with tokenized subwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples, label_all_tokens=True):\n",
    "    \"\"\"\n",
    "    Function to tokenize and align labels with respect to the tokens. This function is specifically designed for\n",
    "    Named Entity Recognition (NER) tasks where alignment of the labels is necessary after tokenization.\n",
    "\n",
    "    Parameters:\n",
    "    examples (dict): A dictionary containing the tokens and the corresponding NER tags.\n",
    "                     - \"tokens\": list of words in a sentence.\n",
    "                     - \"ner_tags\": list of corresponding entity tags for each word.\n",
    "\n",
    "    label_all_tokens (bool): A flag to indicate whether all tokens should have labels.\n",
    "                             If False, only the first token of a word will have a label,\n",
    "                             the other tokens (subwords) corresponding to the same word will be assigned -100.\n",
    "\n",
    "    Returns:\n",
    "    tokenized_inputs (dict): A dictionary containing the tokenized inputs and the corresponding labels aligned with the tokens.\n",
    "    \"\"\"\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        # word_ids() => Return a list mapping the tokens\n",
    "        # to their actual word in the initial sentence.\n",
    "        # It Returns a list indicating the word corresponding to each token.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        # Special tokens like `<s>` and `<\\s>` are originally mapped to None\n",
    "        # We need to set the label to -100 so they are automatically ignored in the loss function.\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                # set –100 as the label for these special tokens\n",
    "                label_ids.append(-100)\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                # if current word_idx is != prev then its the most regular case\n",
    "                # and add the corresponding token\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                # to take care of sub-words which have the same word_idx\n",
    "                # set -100 as well for them, but only if label_all_tokens == False\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "                # mask the subword representations after the first subword\n",
    "\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displaying tokens and their corresponding aligned NER labels after tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 2000, 11598, 1996, 8122, 1997, 1996, 2373, 22686, 1010, 2057, 7528, 1037, 12247, 7077, 2478, 2019, 6728, 1011, 23713, 1998, 7594, 1996, 6434, 2783, 2012, 23842, 1012, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[-100, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 3, 0, 3, 0, -100]]}\n"
     ]
    }
   ],
   "source": [
    "q = tokenize_and_align_labels(examples=electrical_ner_dataset['train'][4:5])\n",
    "print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]___________________________________ -100\n",
      "to______________________________________ 0\n",
      "enhance_________________________________ 0\n",
      "the_____________________________________ 0\n",
      "efficiency______________________________ 0\n",
      "of______________________________________ 0\n",
      "the_____________________________________ 0\n",
      "power___________________________________ 0\n",
      "amplifier_______________________________ 1\n",
      ",_______________________________________ 0\n",
      "we______________________________________ 0\n",
      "implemented_____________________________ 0\n",
      "a_______________________________________ 0\n",
      "feedback________________________________ 0\n",
      "loop____________________________________ 0\n",
      "using___________________________________ 0\n",
      "an______________________________________ 0\n",
      "op______________________________________ 1\n",
      "-_______________________________________ 0\n",
      "amp_____________________________________ 2\n",
      "and_____________________________________ 0\n",
      "measured________________________________ 0\n",
      "the_____________________________________ 0\n",
      "output__________________________________ 0\n",
      "current_________________________________ 3\n",
      "at______________________________________ 0\n",
      "3a______________________________________ 3\n",
      "._______________________________________ 0\n",
      "[SEP]___________________________________ -100\n"
     ]
    }
   ],
   "source": [
    "for token, label in zip(tokenizer.convert_ids_to_tokens(q[\"input_ids\"][0]),q[\"labels\"][0]):\n",
    "    print(f\"{token:_<40} {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the tokenize_and_align_labels function to the entire dataset for consistent processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = electrical_ner_dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Tokenized Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the tokenized dataset to disk for reuse in model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets.save_to_disk(OUTPUT_DATASET_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image_classification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
